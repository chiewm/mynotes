# 统计学习三要素

> 方法 = 模型 + 策略 + 算法

## 1.3.1 模型
假设空间用 $\digamma$ 表示，假设空间可以定义为决策函数的集合.

$$
\digamma = \{f \mid Y = f(X) \} 
$$

也可以为条件概率的集合

$$
\digamma = \{P \mid P(Y \mid X) \} 
$$

## 1.3.2 策略
顺势函数：度量模型一次预测的好坏。  
风险函数：度量平均意义下模型预测的好坏。


### 1. 损失函数和风险函数
用损失函数（loss function）或代价函数（cost function）来度量预测错误的程度。  
损失函数是 $f(X)$ 和 $Y$ 的非负实值函数看，记作 $L(Y, f(X))$ 。

* 0-1 损失函数（0-1 loss function）

$$ 
L(Y,f(X)) = 
\begin{cases} 
& 1, \ Y \neq f(X)  \\
& 0, \ Y  =  f(X)
\end{cases}
$$  


* 平方损失函数（quadratic loss function）

$$
L(Y, f(X)) = (Y - f(X))^2
$$


* 绝对损失函数（absolute loss function）

$$
L(Y, f(x)) = \left | Y - f(X) \right |
$$

* 对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）

$$
L(Y, P(Y \mid X)) = -logP(Y \mid X)
$$

损失函数数值越小，模型就越好。  
损失函数期望是：

$$
R_{exp}(f) = E_{p} \left [ L(Y, f(X)) \right ] = \int_{x \times y}L(y, f(x))P(x, y)dxdy
$$

理论上模型 $f(X)$ 关于联合分布 $P(X, Y)$ 的平均意义下的损失，称为风险函数（risk function）或期望损失（expected loss）


 给定一个训练数据集

 $$
 T = \{ (x_1, y_1), (x_2, y_2), ^{...}, (x_{_N},y_{_N}) \}
 $$

 模型 $f(X)$ 关于训练数据集的平均损失称为经验风险（empirical risk）或经验损失（empirical loss），记作 $R_{emp}$:

$$
R_{emp}(f) = \frac {1} {N} \sum ^N _{i=1} L (y_i, f(x_i))
$$

### 2. 经验风险最小化与结构风险最小化

经验风险最小化（empirical risk minimization， ERM）：

$$
\min_{f \in \digamma} \ \frac {1} {N} \sum ^N _{i=1}L(y_i, f(x_i))
$$

例如极大似然估计（maximum likelihood estimation）


结构风险最小化（structural risk minimization， SRM），等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term）。


$$
R_{emp}(f) = \frac {1} {N} \sum ^N _{i=1} L (y_i, f(x_i)) + \lambda J (f)
$$

其中 $J(f)$ 为模型的复杂度，是定义在假设空间的 $\digamma$ 上的泛函。模型 $f$ 越复杂，复杂度 $J(f)$ 就越大，复杂度表示对复杂模型的惩罚。$\lambda \geqslant 0$ 是系数，用以权衡经验风险和模型复杂度，结构风险小需要经验风险与模型复杂度同时小。

$$
\min_{f \in \digamma} \frac {1} {N} \sum ^N _{i=1} L (y_i, f(x_i)) + \lambda J (f)
$$


## 1.3.3 算法
统计学习问题归结为最优化问题，算法归结为最优化算法。