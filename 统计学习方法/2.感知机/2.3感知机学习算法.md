# 2.3 感知机学习方法

## 2.3.1 感知机学习算法的原始形式
求损失函数极小化问题的解

$$
\underset{w,b}{min}(w, b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b )
$$

其中 $M$ 为误分类的集合。

感知机学习算法是误分类驱动的，采用随机梯度下降法（stochastic gradient descent）。首先，任取一个超平面 $w_0,b_0$,然后用梯度下降法极小化目标函数，极小化过程不是一次使 $M$ 中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。

假设误分类点是固定的，那么损失函数$L(w, b)$的梯度由：

$$
\triangledown _wL(w,b) = -\sum_{x_i \in M} y_ix_i
$$

$$
\triangledown _bL(w,b) = -\sum_{x_i \in M} y_i
$$

给出。

随机选取一个误分类点$(x_i,y_i)$,对$w,b$进行更新：

$$
w \leftarrow w + \eta y_i x_i 
$$

$$
b \leftarrow b + \eta y_i
$$

$\eta(0<\eta\leqslant 1)$ 是步长，又称学习率（learning rate）。

**算法 2.1**（感知机学习算法的原始形式）  
输入：训练集 $T$;  
输出：$w, b$;感知机模型 $f(x) = sign(w \cdot x + b)$  
(1) 选取初值 $w_0, b_0$  
(2) 在训练集中选取数据$(x_i,y_i)$  
(3) 如果$y_i(w \cdot x_i + b) \leqslant 0$，对$w,b$ 更新。   
(4) 转至(2)，直至训练集没有误分类点。

解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w, b$ 的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。

采用不同的初值或选取不同的误分类点，解可以不同。

# 2.3.2 算法的收敛性
略

# 2.3.3 感知机学习算法的对偶形式
**算法2.2** （感知机学习算法的对偶形式）  
输入：线性可分的数据集$T$;
输出：$a, b$;感知机模型$f(x) = sign(\sum_{j=1} ^{N} \alpha_jy_jx_j \cdot x + b)$.  
其中 $\alpha=(\alpha _1, \alpha _2,... \alpha _N) ^T$  
(1) $\alpha \leftarrow 0, b \leftarrow 0$  
(2) 在训练集中选取数据（x_i, y_i）  
(3) 如果 $y_i ({\sum_{j=1} ^{N} \alpha_jy_jx_j \cdot x + b}) \leqslant 0$

$$
\alpha _i \leftarrow \alpha _i + \eta
$$
$$
b  \leftarrow b + \eta y_i
$$

\
(4) 转至（2）知道没有误分类数据。

对偶形式中训练实例以内积的形式出现，可以预先将训练集中实例间的内积计算出来并以举证的形式存储，就是 $Gram$ 矩阵（Gram matrix）

$$
G = [x_i \cdot x_j] _{_{N \times N}}
$$