# 2.3 感知机学习方法

## 2.3.1 感知机学习算法的原始形式
求损失函数极小化问题的解

$$
\underset{w,b}{min}(w, b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b )
$$

其中 $M$ 为误分类的集合。

感知机学习算法是误分类驱动的，采用随机梯度下降法（stochastic gradient descent）。首先，任取一个超平面 $w_0,b_0$,然后用梯度下降法极小化目标函数，极小化过程不是一次使 $M$ 中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。

假设误分类点是固定的，那么损失函数$L(w, b)$的梯度由：

$$
\triangledown _wL(w,b) = -\sum_{x_i \in M} y_ix_i
$$

$$
\triangledown _bL(w,b) = -\sum_{x_i \in M} y_i
$$

给出。

随机选取一个误分类点$(x_i,y_i)$,对$w,b$进行更新：

$$
w \leftarrow w + \eta y_i x_i 
$$

$$
b \leftarrow b + \eta y_i
$$

$\eta(0<\eta\leqslant 1)$ 是步长，又称学习率（learning rate）。

**算法 2.1**（感知机学习算法的原始形式）  
输入：训练集 $T$;  
输出：$w, b$;感知机模型 $f(x) = sign(w \cdot x + b)$  
(1) 选取初值 $w_0, b_0$  
(2) 在训练集中选取数据$(x_i,y_i)$  
(3) 如果$y_i(w \cdot x_i + b) \leqslant 0$，对$w,b$ 更新。   
(4) 转至(2)，直至训练集没有误分类点。

解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w, b$ 的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。